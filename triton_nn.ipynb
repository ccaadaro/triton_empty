{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def matmul(a, b):\n",
    "    \"\"\"\n",
    "    a: 2D tensor of shape (M, K)\n",
    "    b: 2D tensor of shape (K, N)\n",
    "    Returns a new tensor of shape (M, N) in float16.\n",
    "    \"\"\"\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    _, N = b.shape\n",
    "\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "\n",
    "    BLOCK_SIZE_M = 128\n",
    "    BLOCK_SIZE_N = 128\n",
    "    BLOCK_SIZE_K = 32\n",
    "    GROUP_SIZE_M = 8  \n",
    "\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N'])\n",
    "    ,)\n",
    "\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        BLOCK_SIZE_M=BLOCK_SIZE_M,\n",
    "        BLOCK_SIZE_N=BLOCK_SIZE_N,\n",
    "        BLOCK_SIZE_K=BLOCK_SIZE_K,\n",
    "        GROUP_SIZE_M=GROUP_SIZE_M\n",
    "            )\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "B = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "\n",
    "for _ in range(3):\n",
    "    matmul(A, B)\n",
    "\n",
    "import torch._dynamo as dynamo\n",
    "\n",
    "def benchmark():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    torch.matmul(A, B) \n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"PyTorch Vanilla: {start.elapsed_time(end):.3f} ms\")\n",
    "\n",
    "    compiled_fn = dynamo.optimize(\"inductor\")(lambda x, y: x @ y)\n",
    "    for _ in range(3):\n",
    "        compiled_fn(A, B)\n",
    "    start.record()\n",
    "    compiled_fn(A, B)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"TorchInductor: {start.elapsed_time(end):.3f} ms\")\n",
    "\n",
    "    for _ in range(3):\n",
    "        matmul(A, B)\n",
    "    start.record()\n",
    "    matmul(A, B)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"Triton: {start.elapsed_time(end):.3f} ms\")\n",
    "\n",
    "benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_dataset = datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def conv2d_fwd_naive_kernel(\n",
    "    input_ptr, weight_ptr, bias_ptr, output_ptr,\n",
    "    B, C_in, H_in, W_in,\n",
    "    C_out, H_out, W_out,\n",
    "    K_h, K_w,\n",
    "    stride_h, stride_w,\n",
    "    pad_h, pad_w\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    w_out_idx = pid % W_out\n",
    "    pid //= W_out\n",
    "    h_out_idx = pid % H_out\n",
    "    pid //= H_out\n",
    "    c_out_idx = pid % C_out\n",
    "    pid //= C_out\n",
    "    n = pid\n",
    "    if n >= B:\n",
    "        return\n",
    "\n",
    "    out_val = 0.0\n",
    "    in_h_start = h_out_idx*stride_h - pad_h\n",
    "    in_w_start = w_out_idx*stride_w - pad_w\n",
    "\n",
    "    for c_in_idx in range(C_in):\n",
    "        for r in range(K_h):\n",
    "            for c in range(K_w):\n",
    "                in_h = in_h_start + r\n",
    "                in_w = in_w_start + c\n",
    "                in_bounds = (in_h >= 0) & (in_h < H_in) & (in_w >= 0) & (in_w < W_in)\n",
    "                in_offset = (n*C_in*H_in*W_in\n",
    "                             + c_in_idx*H_in*W_in\n",
    "                             + in_h*W_in\n",
    "                             + in_w)\n",
    "                w_offset = (c_out_idx*C_in*K_h*K_w\n",
    "                            + c_in_idx*K_h*K_w\n",
    "                            + r*K_w\n",
    "                            + c)\n",
    "                val_in = tl.load(input_ptr + in_offset, mask=in_bounds, other=0.0)\n",
    "                val_w  = tl.load(weight_ptr + w_offset)\n",
    "                out_val += val_in * val_w\n",
    "\n",
    "    bias_val = tl.load(bias_ptr + c_out_idx)\n",
    "    out_val += bias_val\n",
    "\n",
    "    out_offset = (n*C_out*H_out*W_out\n",
    "                  + c_out_idx*H_out*W_out\n",
    "                  + h_out_idx*W_out\n",
    "                  + w_out_idx)\n",
    "    tl.store(output_ptr + out_offset, out_val)\n",
    "\n",
    "def triton_conv2d_naive(input, weight, bias=None, stride=1, padding=0):\n",
    "    if isinstance(stride, int):\n",
    "        stride_h, stride_w = stride, stride\n",
    "    else:\n",
    "        stride_h, stride_w = stride\n",
    "    if isinstance(padding, int):\n",
    "        pad_h, pad_w = padding, padding\n",
    "    else:\n",
    "        pad_h, pad_w = padding\n",
    "\n",
    "    B, C_in, H_in, W_in = input.shape\n",
    "    C_out, C_in2, K_h, K_w = weight.shape\n",
    "    assert C_in == C_in2\n",
    "\n",
    "    H_out = (H_in + 2*pad_h - K_h)//stride_h + 1\n",
    "    W_out = (W_in + 2*pad_w - K_w)//stride_w + 1\n",
    "\n",
    "    out = torch.empty((B, C_out, H_out, W_out), device=input.device, dtype=input.dtype)\n",
    "    in_ptr  = input.flatten()\n",
    "    w_ptr   = weight.flatten()\n",
    "    b_ptr   = bias.flatten() if bias is not None else 0\n",
    "    out_ptr = out.flatten()\n",
    "\n",
    "    num_out_elems = B*C_out*H_out*W_out\n",
    "    grid = lambda meta: (num_out_elems,)\n",
    "    conv2d_fwd_naive_kernel[grid](\n",
    "        in_ptr, w_ptr, b_ptr, out_ptr,\n",
    "        B, C_in, H_in, W_in,\n",
    "        C_out, H_out, W_out,\n",
    "        K_h, K_w,\n",
    "        stride_h, stride_w,\n",
    "        pad_h, pad_w\n",
    "    )\n",
    "    return out\n",
    "\n",
    "@triton.jit\n",
    "def relu_kernel(x_ptr, y_ptr, N, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(0)\n",
    "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < N\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.where(x > 0, x, 0)\n",
    "    tl.store(y_ptr + offsets, y, mask=mask)\n",
    "\n",
    "def triton_relu(x):\n",
    "    out = torch.empty_like(x)\n",
    "    N = x.numel()\n",
    "    BLOCK_SIZE = 1024  \n",
    "    \n",
    "    grid = lambda meta: ((N + BLOCK_SIZE - 1)//BLOCK_SIZE, )\n",
    "\n",
    "    relu_kernel[grid](x, out, N, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    return out\n",
    "\n",
    "\n",
    "class TritonCNNModel(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(TritonCNNModel, self).__init__()\n",
    "\n",
    "        # conv1: (16,1,3,3)\n",
    "        self.conv1_weight = nn.Parameter(torch.randn(16, in_channels, 3, 3)*0.01)\n",
    "        self.conv1_bias   = nn.Parameter(torch.zeros(16))\n",
    "        # conv2: (32,16,3,3)\n",
    "        self.conv2_weight = nn.Parameter(torch.randn(32,16,3,3)*0.01)\n",
    "        self.conv2_bias   = nn.Parameter(torch.zeros(32))\n",
    "\n",
    "        # final FC layers\n",
    "        self.fc1 = nn.Linear(32*24*24, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = triton_conv2d_naive(x, self.conv1_weight, self.conv1_bias, stride=1, padding=0)\n",
    "        x = triton_relu(x)\n",
    "        x = triton_conv2d_naive(x, self.conv2_weight, self.conv2_bias, stride=1, padding=0)\n",
    "        x = triton_relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model_triton = TritonCNNModel().to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    model_triton.conv1_weight.copy_(model_pt.conv1.weight)\n",
    "    model_triton.conv1_bias.copy_(model_pt.conv1.bias)\n",
    "\n",
    "    model_triton.conv2_weight.copy_(model_pt.conv2.weight)\n",
    "    model_triton.conv2_bias.copy_(model_pt.conv2.bias)\n",
    "\n",
    "    model_triton.fc1.weight.copy_(model_pt.fc1.weight)\n",
    "    model_triton.fc1.bias.copy_(model_pt.fc1.bias)\n",
    "\n",
    "    model_triton.fc2.weight.copy_(model_pt.fc2.weight)\n",
    "    model_triton.fc2.bias.copy_(model_pt.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "data, target = next(iter(test_loader))\n",
    "data = data.to(device)\n",
    "\n",
    "for _ in range(5):\n",
    "    _ = model_pt(data)\n",
    "    _ = model_triton(data)\n",
    "\n",
    "torch.cuda.synchronize()  \n",
    "\n",
    "t0 = time.time()\n",
    "_ = model_pt(data)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "pytorch_time = t1 - t0\n",
    "\n",
    "t2 = time.time()\n",
    "_ = model_triton(data)\n",
    "torch.cuda.synchronize()\n",
    "t3 = time.time()\n",
    "triton_time = t3 - t2\n",
    "\n",
    "print(f\"PyTorch forward pass time: {pytorch_time*1e3:.3f} ms\")\n",
    "print(f\"Triton forward pass time:  {triton_time*1e3:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "\n",
    "\n",
    "def dropout_relu_scale_pytorch(x, p=0.5, alpha=1.0):\n",
    "    mask = (torch.rand_like(x) > p).float() / (1-p)\n",
    "    y = x * mask        # dropout\n",
    "    y = torch.relu(y)   # relu\n",
    "    z = alpha * y       # scale\n",
    "    return z\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def fused_dropout_relu_scale_kernel(\n",
    "    x_ptr, out_ptr, seed_ptr,  \n",
    "    n_elements,\n",
    "    p, alpha,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "\n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
    "\n",
    "\n",
    "    rng_offset = tl.load(seed_ptr) + offsets  \n",
    "    random_val = (rng_offset % 10000) / 10000  \n",
    "    keep = random_val > p\n",
    "    scale = 1.0 / (1.0 - p)\n",
    "    x = x * keep * scale  \n",
    "\n",
    "    # ReLU\n",
    "    x = tl.where(x > 0, x, 0.0)\n",
    "\n",
    "    # scale by alpha\n",
    "    x = x * alpha\n",
    "\n",
    "    # store out\n",
    "    tl.store(out_ptr + offsets, x, mask=mask)\n",
    "\n",
    "def fused_dropout_relu_scale_triton(x, p=0.5, alpha=1.0):\n",
    "    \"\"\"One-pass fused dropout+ReLU+scale using Triton.\"\"\"\n",
    "    out = torch.empty_like(x)\n",
    "    n_elements = x.numel()\n",
    "\n",
    "\n",
    "    seed = torch.randint(0, 2**10, (1,), device=x.device, dtype=torch.int32)\n",
    "\n",
    "    BLOCK_SIZE = 1024\n",
    "    grid = lambda meta: ((n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE, )\n",
    "\n",
    "    fused_dropout_relu_scale_kernel[grid](\n",
    "        x, out, seed,  \n",
    "        n_elements,\n",
    "        p, alpha,\n",
    "        BLOCK_SIZE=BLOCK_SIZE\n",
    "    )\n",
    "    return out\n",
    "\n",
    "import time\n",
    "\n",
    "x = torch.randn(10_000_000, device='cuda')  \n",
    "\n",
    "# Warm-up\n",
    "for _ in range(2):\n",
    "    _ = dropout_relu_scale_pytorch(x, p=0.5, alpha=1.23)\n",
    "    _ = fused_dropout_relu_scale_triton(x, p=0.5, alpha=1.23)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.time()\n",
    "y_pt = dropout_relu_scale_pytorch(x, p=0.5, alpha=1.23)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "pt_time = t1 - t0\n",
    "\n",
    "t2 = time.time()\n",
    "y_tt = fused_dropout_relu_scale_triton(x, p=0.5, alpha=1.23)\n",
    "torch.cuda.synchronize()\n",
    "t3 = time.time()\n",
    "tt_time = t3 - t2\n",
    "\n",
    "print(f\"PyTorch time: {pt_time*1e3:.3f} ms\")\n",
    "print(f\"Triton time:  {tt_time*1e3:.3f} ms\")\n",
    "\n",
    "print(\"Max diff:\", (y_pt - y_tt).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "###############################\n",
    "# 1. PyTorch reference version\n",
    "###############################\n",
    "def dropout_relu_scale_pytorch(x, p=0.5, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "      1) Dropout\n",
    "      2) ReLU\n",
    "      3) scale by alpha\n",
    "    Each step is potentially a separate kernel in PyTorch.\n",
    "    \"\"\"\n",
    "    # dropout\n",
    "    mask = (torch.rand_like(x) > p).float() / (1.0 - p)\n",
    "    y = x * mask\n",
    "    # relu\n",
    "    y = torch.relu(y)\n",
    "    # scale\n",
    "    y = alpha * y\n",
    "    return y\n",
    "\n",
    "\n",
    "###############################\n",
    "# 2. Triton fused kernel\n",
    "###############################\n",
    "\n",
    "@triton.jit\n",
    "def fused_dropout_relu_scale_kernel(\n",
    "    x_ptr, out_ptr,\n",
    "    seed_int,     # <-- This will be an actual int, not a pointer\n",
    "    n_elements,\n",
    "    p, alpha,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
    "\n",
    "    # Now seed_int is a scalar int, so pointer+int mismatch is gone\n",
    "    rng_offset = seed_int + offsets\n",
    "    rand_val = (rng_offset % 100000) / 100000.0\n",
    "    keep = rand_val > p\n",
    "    scale = 1.0 / (1.0 - p)\n",
    "    x = x * keep * scale\n",
    "    x = tl.where(x > 0, x, 0.0)\n",
    "    x = alpha * x\n",
    "\n",
    "    tl.store(out_ptr + offsets, x, mask=mask)\n",
    "\n",
    "def fused_dropout_relu_scale_triton(x, p=0.5, alpha=1.0):\n",
    "    out = torch.empty_like(x)\n",
    "    n_elements = x.numel()\n",
    "    # Convert single-element tensor to a plain int\n",
    "    seed_val = torch.randint(0, 2**20, (1,), device='cuda', dtype=torch.int32).item()\n",
    "\n",
    "    BLOCK_SIZE = 1024\n",
    "    grid = lambda meta: ((n_elements + BLOCK_SIZE - 1)//BLOCK_SIZE, )\n",
    "\n",
    "    fused_dropout_relu_scale_kernel[grid](\n",
    "        x, out,\n",
    "        seed_val,        # pass an int, not a tensor\n",
    "        n_elements,\n",
    "        p, alpha,\n",
    "        BLOCK_SIZE=BLOCK_SIZE\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "###############################\n",
    "# 3. Benchmark with torch.utils.benchmark\n",
    "###############################\n",
    "\n",
    "def run_benchmark():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.randn(10_000_000, device=device)  # 50 million floats\n",
    "    p = 0.5\n",
    "    alpha = 1.23\n",
    "\n",
    "    # Warm up to let GPU reach stable states\n",
    "    for _ in range(5):\n",
    "        dropout_relu_scale_pytorch(x, p, alpha)\n",
    "        fused_dropout_relu_scale_triton(x, p, alpha)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # We'll create two benchmark timers\n",
    "    t_pytorch = benchmark.Timer(\n",
    "        stmt=\"dropout_relu_scale_pytorch(x, 0.5, 1.23)\",\n",
    "        setup=\"from __main__ import dropout_relu_scale_pytorch, x\",\n",
    "        label=\"Dropout+ReLU+Scale\",\n",
    "        sub_label=\"PyTorch multiple kernels\",\n",
    "    )\n",
    "\n",
    "    t_triton = benchmark.Timer(\n",
    "        stmt=\"fused_dropout_relu_scale_triton(x, 0.5, 1.23)\",\n",
    "        setup=\"from __main__ import fused_dropout_relu_scale_triton, x\",\n",
    "        label=\"Dropout+ReLU+Scale\",\n",
    "        sub_label=\"Triton fused kernel\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # We can do multiple runs for each\n",
    "    num_runs = 5\n",
    "    results_pytorch = t_pytorch.blocked_autorange(min_run_time=1)\n",
    "    results_triton  = t_triton.blocked_autorange(min_run_time=1)\n",
    "\n",
    "    print(results_pytorch)\n",
    "    print(results_triton)\n",
    "\n",
    "    # We can compare the times\n",
    "    print(\"PyTorch time (median):\", results_pytorch.median)\n",
    "    print(\"Triton  time (median):\", results_triton.median)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
